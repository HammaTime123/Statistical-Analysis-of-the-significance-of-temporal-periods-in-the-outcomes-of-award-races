{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import relevent packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, ensure that any previous installations or upgrades are reflected in the current kernel\n",
    "# by using the `pip` magic command.\n",
    "\n",
    "def upgrade_pip():\n",
    "    \"\"\"\n",
    "    Upgrades pip to the latest version to ensure compatibility and access to the latest packages.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(\"Upgrading pip to the latest version...\")\n",
    "        # Use the pip magic command to upgrade pip\n",
    "        !{sys.executable} -m pip install --upgrade pip\n",
    "        print(\"Successfully upgraded pip.\\n\")\n",
    "    except Exception as upgrade_error:\n",
    "        print(f\"Failed to upgrade pip: {upgrade_error}\")\n",
    "        print(\"Continuing with existing pip version.\\n\")\n",
    "\n",
    "def install_packages(package_list):\n",
    "    \"\"\"\n",
    "    Attempts to install the specified packages using pip.\n",
    "    If pip fails, it retries using pip3.\n",
    "\n",
    "    Parameters:\n",
    "        package_list (list): A list of package names to install.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"Attempting to install packages {package_list} using pip...\")\n",
    "        # Use the pip magic command to install packages\n",
    "        !{sys.executable} -m pip install {\" \".join(package_list)}\n",
    "        print(f\"Successfully installed packages using pip.\\n\")\n",
    "    except Exception as pip_error:\n",
    "        print(f\"pip installation failed: {pip_error}\")\n",
    "        print(f\"Attempting to install packages {package_list} using pip3...\")\n",
    "        try:\n",
    "            # Attempt to install using pip3\n",
    "            !pip3 install {\" \".join(package_list)}\n",
    "            print(f\"Successfully installed packages using pip3.\\n\")\n",
    "        except Exception as pip3_error:\n",
    "            print(f\"pip3 installation failed: {pip3_error}\")\n",
    "            print(f\"Failed to install packages using both pip and pip3.\")\n",
    "            raise RuntimeError(\"Package installation failed.\") from pip3_error\n",
    "\n",
    "def verify_imports(import_statements):\n",
    "    \"\"\"\n",
    "    Attempts to import each specified module and function to verify successful installation.\n",
    "\n",
    "    Parameters:\n",
    "        import_statements (list): A list of import statements as strings.\n",
    "    \"\"\"\n",
    "    print(\"Verifying package installations by importing them...\")\n",
    "    for stmt in import_statements:\n",
    "        try:\n",
    "            exec(stmt)\n",
    "            print(f\"Successfully executed: {stmt}\")\n",
    "        except ImportError as import_error:\n",
    "            print(f\"Failed to execute '{stmt}': {import_error}\")\n",
    "            raise ImportError(f\"Import failed for statement: {stmt}\") from import_error\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while executing '{stmt}': {e}\")\n",
    "            raise\n",
    "    print(\"All specified imports executed successfully.\\n\")\n",
    "\n",
    "def setUpEnvironment():\n",
    "    # List of packages to install\n",
    "    packages_to_install = [\n",
    "        \"basketball-reference-scraper\",\n",
    "        \"pandas\",\n",
    "        \"numpy\",\n",
    "        \"scikit-learn\",\n",
    "    ]\n",
    "\n",
    "    # List of import statements to verify installations\n",
    "    import_statements = [\n",
    "        \"from basketball_reference_scraper.teams import get_roster, get_team_stats, get_opp_stats, get_roster_stats, get_team_misc\",\n",
    "        \"from basketball_reference_scraper.players import get_stats, get_game_logs\",\n",
    "        \"import pandas as pd\",\n",
    "        \"import sklearn\",\n",
    "        \"from datetime import timedelta\",\n",
    "        \"from datetime import datetime\"\n",
    "    ]\n",
    "\n",
    "    # Upgrade pip before attempting installations\n",
    "    upgrade_pip()\n",
    "\n",
    "    # Install the required packages\n",
    "    install_packages(packages_to_install)\n",
    "\n",
    "    # # Verify installations by executing import statements\n",
    "    # verify_imports(import_statements)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upgrading pip to the latest version...\n",
      "zsh:1: parse error near `-m'\n",
      "Successfully upgraded pip.\n",
      "\n",
      "Attempting to install packages ['basketball-reference-scraper', 'pandas', 'numpy', 'scikit-learn'] using pip...\n",
      "zsh:1: parse error near `-m'\n",
      "Successfully installed packages using pip.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "setUpEnvironment()\n",
    "from basketball_reference_scraper.players import get_stats, get_game_logs\n",
    "from basketball_reference_scraper.teams import get_roster, get_team_stats, get_opp_stats, get_roster_stats, get_team_misc\n",
    "import pandas as pd\n",
    "from datetime import timedelta, datetime\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, MaxAbsScaler\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "from contextlib import redirect_stdout, redirect_stderr\n",
    "from IPython.display import clear_output\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, MaxAbsScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "RED = \"\\033[91m\"\n",
    "GREEN = \"\\033[92m\"\n",
    "YELLOW = \"\\033[93m\"\n",
    "BLUE = \"\\033[94m\"\n",
    "MAGENTA = \"\\033[95m\"\n",
    "CYAN = \"\\033[96m\"\n",
    "RESET = \"\\033[0m\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example import statement for a players game by game stats for a given season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_jupyter():\n",
    "    clear_output(wait=True)\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def get_player_season(player_name, season_year: int, include_playoffs: bool, position: int):\n",
    "    # Scrape for data\n",
    "    df = pd.DataFrame(get_game_logs(player_name, season_year, playoffs=include_playoffs))\n",
    "    clear_jupyter()\n",
    "    df.to_csv(\"tester.csv\", index=False)\n",
    "    \n",
    "    # Choose the columns you are interested in\n",
    "    columns_to_keep = [\"DATE\", \"PTS\", \"AST\", \"TRB\"]\n",
    "    df_selected = df.loc[:, columns_to_keep]\n",
    "    \n",
    "    # Convert PTS, AST, TRB to numeric, coercing errors to NaN\n",
    "    df_numeric = df_selected[['PTS', 'AST', 'TRB']].apply(pd.to_numeric, errors='coerce')\n",
    "    \n",
    "    # Create a mask where all PTS, AST, TRB are not NaN (i.e., are numeric)\n",
    "    mask = df_numeric.notnull().all(axis=1)\n",
    "    \n",
    "    # Apply the mask to filter out rows with non-numeric values\n",
    "    df_cleaned = df_selected[mask].reset_index(drop=True)\n",
    "    \n",
    "    # Add the player's name and position using .loc\n",
    "    df_cleaned.loc[:, 'Player'] = player_name\n",
    "    df_cleaned.loc[:, 'Position'] = position\n",
    "    \n",
    "    return df_cleaned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_player_stats(players, target_year, include_playoffs=False, output_file=\"MVP_stats.csv\"):\n",
    "    \"\"\"\n",
    "    Collects season statistics for a list of players with logging and progress indicators,\n",
    "    and saves the combined data to a CSV file.\n",
    "\n",
    "    Parameters:\n",
    "    - players (list of str): List of player names to fetch stats for.\n",
    "    - target_year (int): The target year for which to fetch stats.\n",
    "    - include_playoffs (bool): Whether to include playoff stats. Default is False.\n",
    "    - output_file (str): The filename for the output CSV. Default is \"MVP_stats.csv\".\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: Combined DataFrame containing all players' stats.\n",
    "    \"\"\"\n",
    "    \n",
    "    # ANSI escape codes for green color\n",
    "    GREEN = \"\\033[92m\"\n",
    "    RESET = \"\\033[0m\"\n",
    "    \n",
    "    # Define a custom bar format with green color for the progress bar\n",
    "    custom_bar_format = (\n",
    "        \"{l_bar}\"\n",
    "        f\"{GREEN}\"  # Start green color\n",
    "        \"{bar}\"\n",
    "        f\"{RESET}\"  # Reset color\n",
    "        \"| {n_fmt}/{total_fmt} [{elapsed}<{remaining}]\"\n",
    "    )\n",
    "    \n",
    "    # Initialize a list to store individual player DataFrames\n",
    "    mvp_stats_list = []\n",
    "\n",
    "    # Iterate over each player with a green progress bar\n",
    "    for intTicker, player in tqdm(\n",
    "        enumerate(players, start=1),\n",
    "        total=len(players),\n",
    "        desc=\"Fetching Player Stats\",\n",
    "        bar_format=custom_bar_format\n",
    "    ):\n",
    "        try:\n",
    "            # Fetch the player's season statistics\n",
    "            player_stats = get_player_season(player, target_year, include_playoffs, intTicker)\n",
    "            # Append the player's stats to the list\n",
    "            mvp_stats_list.append(player_stats)\n",
    "            \n",
    "            # Log the successful fetch\n",
    "            logging.info(f\"Successfully fetched stats for {player}.\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            # Log any errors encountered\n",
    "            logging.error(f\"Error fetching stats for {player}: {e}\")\n",
    "            continue  # Skip to the next player in case of an error\n",
    "\n",
    "    # Concatenate all player stats into a single DataFrame\n",
    "    if mvp_stats_list:\n",
    "        try:\n",
    "            mvp_stats = pd.concat(mvp_stats_list, ignore_index=True)\n",
    "            # Save the combined DataFrame to a CSV file\n",
    "            mvp_stats.to_csv(output_file, index=False)\n",
    "            logging.info(f\"Combined statistics saved to {output_file}.\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error saving DataFrame to CSV: {e}\")\n",
    "            mvp_stats = pd.DataFrame()  # Return an empty DataFrame in case of failure\n",
    "    else:\n",
    "        mvp_stats = pd.DataFrame()\n",
    "        logging.warning(\"No player stats were fetched. Returning an empty DataFrame.\")\n",
    "\n",
    "    return mvp_stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, MaxAbsScaler\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "def normalize_player_stats(df, method='min-max'):\n",
    "    \"\"\"\n",
    "    Normalizes the 'PTS', 'AST', and 'TRB' columns in the DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The input DataFrame containing player statistics.\n",
    "    - method (str): Normalization method - 'min-max', 'z-score', or 'max-abs'.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: DataFrame with normalized 'PTS', 'AST', and 'TRB' columns.\n",
    "    \"\"\"\n",
    "    # ANSI escape codes for blue color\n",
    "    BLUE = \"\\033[94m\"\n",
    "    RESET = \"\\033[0m\"\n",
    "    \n",
    "    # Define a custom bar format with blue color for the progress bar\n",
    "    custom_bar_format = (\n",
    "        \"{l_bar}\"\n",
    "        f\"{GREEN}\"  # Start blue color\n",
    "        \"{bar}\"\n",
    "        f\"{RESET}\"  # Reset color\n",
    "        \" | {n_fmt}/{total_fmt} [{elapsed}<{remaining}]\"\n",
    "    )\n",
    "    \n",
    "    # Define the columns to normalize\n",
    "    columns_to_normalize = ['PTS', 'AST', 'TRB']\n",
    "    \n",
    "    # Check if the necessary columns exist in the DataFrame\n",
    "    for col in columns_to_normalize:\n",
    "        if col not in df.columns:\n",
    "            raise KeyError(f\"Column '{col}' not found in DataFrame.\")\n",
    "    \n",
    "    # Choose the normalization method\n",
    "    if method == 'min-max':\n",
    "        scaler = MinMaxScaler()\n",
    "    elif method == 'z-score':\n",
    "        scaler = StandardScaler()\n",
    "    elif method == 'max-abs':\n",
    "        scaler = MaxAbsScaler()\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported normalization method. Choose 'min-max', 'z-score', or 'max-abs'.\")\n",
    "    \n",
    "    # Iterate over each column with a blue progress bar\n",
    "    for col in tqdm(\n",
    "        columns_to_normalize, \n",
    "        desc=\"Normalizing Columns\", \n",
    "        unit=\"column\",\n",
    "        bar_format=custom_bar_format\n",
    "    ):\n",
    "        # Reshape the data for the scaler and overwrite the column with normalized values\n",
    "        df[col] = scaler.fit_transform(df[[col]])\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:22: SyntaxWarning: invalid escape sequence '\\e'\n",
      "<>:22: SyntaxWarning: invalid escape sequence '\\e'\n",
      "/var/folders/lm/cdbfkpsd6sv7chf63mqhbpfc0000gn/T/ipykernel_1806/290466258.py:22: SyntaxWarning: invalid escape sequence '\\e'\n",
      "  RED = \"\\e[0;36m\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datetime import timedelta\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "def separate_by_date(df, save_paths=None):\n",
    "    \"\"\"\n",
    "    Separates the DataFrame into three equal-duration date ranges, prints the divisions,\n",
    "    and optionally saves each division to specified file paths with a progress bar.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The input DataFrame with a 'DATE' column.\n",
    "    save_paths (list of str, optional): A list of three file paths to save the divisions.\n",
    "                                        The list should contain exactly three strings.\n",
    "                                        Example: ['division1.csv', 'division2.csv', 'division3.csv']\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing three DataFrames corresponding to the three date divisions.\n",
    "    \"\"\"\n",
    "    \n",
    "    # ANSI escape codes for red color\n",
    "    RED = \"\\e[0;36m\"\n",
    "    RESET = \"\\033[0m\"\n",
    "    \n",
    "    # Define a custom bar format with red color for the progress bar\n",
    "    custom_bar_format = (\n",
    "        \"{l_bar}\"\n",
    "        f\"{GREEN}\"  # Start red color\n",
    "        \"{bar}\"\n",
    "        f\"{RESET}\"  # Reset color\n",
    "        \"| {n_fmt}/{total_fmt} [{elapsed}<{remaining}]\"\n",
    "    )\n",
    "    \n",
    "    # Ensure the 'DATE' column is in datetime format\n",
    "    df['DATE'] = pd.to_datetime(df['DATE'])\n",
    "    \n",
    "    # Find the minimum and maximum dates in the DataFrame\n",
    "    min_date = df['DATE'].min()\n",
    "    max_date = df['DATE'].max()\n",
    "    \n",
    "    # Calculate the total number of days in the range\n",
    "    total_days = (max_date - min_date).days + 1  # +1 to include both start and end dates\n",
    "    \n",
    "    # Calculate the number of days per division\n",
    "    days_per_division = total_days // 3\n",
    "    remainder_days = total_days % 3  # To handle cases where total_days is not perfectly divisible by 3\n",
    "    \n",
    "    # Define the end dates for each division\n",
    "    first_end = min_date + timedelta(days=days_per_division - 1)\n",
    "    second_end = first_end + timedelta(days=days_per_division)\n",
    "    \n",
    "    # Distribute the remainder_days\n",
    "    if remainder_days == 1:\n",
    "        second_end += timedelta(days=1)\n",
    "    elif remainder_days == 2:\n",
    "        first_end += timedelta(days=1)\n",
    "        second_end += timedelta(days=1)\n",
    "    \n",
    "    # Define the three date ranges\n",
    "    division1 = (df['DATE'] >= min_date) & (df['DATE'] <= first_end)\n",
    "    division2 = (df['DATE'] > first_end) & (df['DATE'] <= second_end)\n",
    "    division3 = (df['DATE'] > second_end) & (df['DATE'] <= max_date)\n",
    "    \n",
    "    # Create separate DataFrames for each division\n",
    "    df_division1 = df[division1].reset_index(drop=True)\n",
    "    df_division2 = df[division2].reset_index(drop=True)\n",
    "    df_division3 = df[division3].reset_index(drop=True)\n",
    "    \n",
    "    # Print the divisions\n",
    "    print(\"Date Divisions:\")\n",
    "    print(f\"Division 1: {min_date.date()} to {first_end.date()}\")\n",
    "    print(f\"Division 2: {first_end.date() + timedelta(days=1)} to {second_end.date()}\")\n",
    "    print(f\"Division 3: {second_end.date() + timedelta(days=1)} to {max_date.date()}\")\n",
    "    \n",
    "    # If save_paths is provided, save each division to the respective path with a progress bar\n",
    "    if save_paths:\n",
    "        if not isinstance(save_paths, list):\n",
    "            raise TypeError(\"save_paths must be a list of three file path strings.\")\n",
    "        if len(save_paths) != 3:\n",
    "            raise ValueError(\"save_paths must contain exactly three file path strings.\")\n",
    "        \n",
    "        # Initialize tqdm progress bar for saving divisions with red color\n",
    "        print(\"\\nSaving Divisions to CSV Files:\")\n",
    "        for i, (division_df, path) in enumerate(tqdm(\n",
    "            zip([df_division1, df_division2, df_division3], save_paths), \n",
    "            total=3, \n",
    "            desc=\"Saving Divisions\", \n",
    "            unit=\"file\",\n",
    "            bar_format=custom_bar_format\n",
    "        ), start=1):\n",
    "            division_df.to_csv(path, index=False)\n",
    "            print(f\"Division {i} saved to {path}\")\n",
    "    \n",
    "    return df_division1, df_division2, df_division3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Player Stats: 100%|\u001b[92m██████████\u001b[0m| 9/9 [00:58<00:00]\n",
      "Normalizing Columns: 100%|\u001b[92m██████████\u001b[0m | 3/3 [00:00<00:00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date Divisions:\n",
      "Division 1: 2023-10-24 to 2023-12-20\n",
      "Division 2: 2023-12-21 to 2024-02-16\n",
      "Division 3: 2024-02-17 to 2024-04-14\n",
      "\n",
      "Saving Divisions to CSV Files:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Divisions: 100%|\u001b[92m██████████\u001b[0m| 3/3 [00:00<00:00]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Division 1 saved to first.csv\n",
      "Division 2 saved to second.csv\n",
      "Division 3 saved to third.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ### 2024 example\n",
    "\n",
    "# players = [\n",
    "#     \"Nikola Jokić\",\n",
    "#     \"Shai Gilgeous-Alexander\",\n",
    "#     \"Luka Dončić\",\n",
    "#     \"Giannis Antetokounmpo\",\n",
    "#     \"Jalen Brunson\",\n",
    "#     \"Jayson Tatum\",\n",
    "#     \"Anthony Edwards\",\n",
    "#     \"Domantas Sabonis\",\n",
    "#     \"Kevin Durant\"\n",
    "# ]\n",
    "\n",
    "# target_year = 2024\n",
    "# include_playoffs = False\n",
    "\n",
    "# ###\n",
    "\n",
    "# mvp_stats = collect_player_stats(players, target_year, include_playoffs, output_file=\"MVP_stats_DIRTY.csv\")\n",
    "# mvp_stats = normalize_player_stats(mvp_stats, 'min-max')\n",
    "# temp1, temp2 , temp3 = separate_by_date(mvp_stats, [\"first.csv\", \"second.csv\", \"third.csv\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(multi_class=&#x27;multinomial&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;LogisticRegression<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.linear_model.LogisticRegression.html\">?<span>Documentation for LogisticRegression</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>LogisticRegression(multi_class=&#x27;multinomial&#x27;)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(multi_class='multinomial')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Define features and target\n",
    "# X = mvp_stats[['PTS', 'AST', 'TRB']]\n",
    "# y = mvp_stats['Position']\n",
    "\n",
    "# # Split the data\n",
    "# X_train, X_test, y_train, y_test = train_test_split(\n",
    "#     X, y, test_size=0.3, random_state=42, stratify=y\n",
    "# )\n",
    "\n",
    "# # Initialize the Logistic Regression model\n",
    "# model = LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
    "\n",
    "# # Train the model\n",
    "# model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Accuracy: 0.41\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 6  1  0  2  1  0  0 14  0]\n",
      " [ 0  1  2  1 11  2  3  0  3]\n",
      " [ 2  0 14  1  2  0  1  1  0]\n",
      " [ 5  0  1  5  1  2  1  7  0]\n",
      " [ 0  0  1  0 19  0  2  1  0]\n",
      " [ 0  0  1  3  3  7  4  0  4]\n",
      " [ 0  0  0  4  5  0 10  0  5]\n",
      " [ 0  0  1  1  0  1  1 21  0]\n",
      " [ 1  2  2  0  5  1  8  1  2]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.43      0.25      0.32        24\n",
      "           2       0.25      0.04      0.07        23\n",
      "           3       0.64      0.67      0.65        21\n",
      "           4       0.29      0.23      0.26        22\n",
      "           5       0.40      0.83      0.54        23\n",
      "           6       0.54      0.32      0.40        22\n",
      "           7       0.33      0.42      0.37        24\n",
      "           8       0.47      0.84      0.60        25\n",
      "           9       0.14      0.09      0.11        22\n",
      "\n",
      "    accuracy                           0.41       206\n",
      "   macro avg       0.39      0.41      0.37       206\n",
      "weighted avg       0.39      0.41      0.37       206\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# # Make predictions\n",
    "# y_pred = model.predict(X_test)\n",
    "\n",
    "# # Calculate accuracy\n",
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "# print(f\"\\nModel Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# # Confusion Matrix\n",
    "# conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "# print(\"\\nConfusion Matrix:\")\n",
    "# print(conf_matrix)\n",
    "\n",
    "# # Classification Report\n",
    "# class_report = classification_report(y_test, y_pred)\n",
    "# print(\"\\nClassification Report:\")\n",
    "# print(class_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights (coefficients): [[-0.05057482  2.12475742  3.16140349]\n",
      " [ 0.91066737 -0.24337766 -2.3828792 ]\n",
      " [ 2.27328011  2.95990346  0.37550413]\n",
      " [ 1.26566077 -0.80514859  2.59703798]\n",
      " [ 0.40833222  0.2199028  -4.58580151]\n",
      " [-0.6149669  -2.11328301  0.09396716]\n",
      " [-0.39411159 -1.12377577 -2.72326125]\n",
      " [-3.73065206  1.04004159  4.47757914]\n",
      " [-0.0676351  -2.05902022 -1.01354993]]\n",
      "Intercepts: [-1.82471956  0.57598704 -2.19922444 -0.97901659  1.18644618  1.07884647\n",
      "  1.52360808 -0.56611492  1.20418774]\n",
      "Feature importance:\n",
      "PTS: 9.715880930050394\n",
      "AST: 12.689210524295387\n",
      "TRB: 21.41098380270971\n",
      "Average feature importance:\n",
      "PTS: 1.079542325561155\n",
      "AST: 1.4099122804772652\n",
      "TRB: 2.3789982003010786\n",
      "Feature importance as percentages:\n",
      "PTS: 22.17%\n",
      "AST: 28.96%\n",
      "TRB: 48.87%\n"
     ]
    }
   ],
   "source": [
    "# # After training the model, extract the weights (coefficients)\n",
    "# weights = model.coef_\n",
    "# intercepts = model.intercept_\n",
    "\n",
    "# # Print the weights and intercepts\n",
    "# print(\"Weights (coefficients):\", weights)\n",
    "# print(\"Intercepts:\", intercepts)\n",
    "\n",
    "# # Calculate the sum of the absolute values of the coefficients for each feature\n",
    "# feature_importance = abs(weights).sum(axis=0)\n",
    "# importance_dict = dict(zip(['PTS', 'AST', 'TRB'], feature_importance))\n",
    "\n",
    "# # Display the importance of each feature\n",
    "# print(\"Feature importance:\")\n",
    "# for feature, importance in importance_dict.items():\n",
    "#     print(f\"{feature}: {importance}\")\n",
    "\n",
    "\n",
    "# # Calculate the average of the absolute values of the coefficients for each feature\n",
    "# feature_importance_avg = abs(weights).mean(axis=0)\n",
    "# importance_avg_dict = dict(zip(['PTS', 'AST', 'TRB'], feature_importance_avg))\n",
    "\n",
    "# # Display the average importance of each feature\n",
    "# print(\"Average feature importance:\")\n",
    "# for feature, importance in importance_avg_dict.items():\n",
    "#     print(f\"{feature}: {importance}\")\n",
    "\n",
    "# # Normalize the importance to get percentages\n",
    "# total_importance = feature_importance.sum()\n",
    "# feature_importance_percentage = {\n",
    "#     feature: (importance / total_importance) * 100\n",
    "#     for feature, importance in importance_dict.items()\n",
    "# }\n",
    "\n",
    "# # Display the percentage contribution of each feature\n",
    "# print(\"Feature importance as percentages:\")\n",
    "# for feature, percentage in feature_importance_percentage.items():\n",
    "#     print(f\"{feature}: {percentage:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def scale_and_sum_stats(df, weights):\n",
    "    \"\"\"\n",
    "    Scales the 'PTS', 'AST', and 'TRB' columns by the provided percentage weights and creates\n",
    "    a new column with the weighted sum.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): DataFrame containing 'PTS', 'AST', 'TRB', and other columns.\n",
    "    - weights (dict): A dictionary with keys 'PTS', 'AST', and 'TRB' and their corresponding weights (in percentage).\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: DataFrame with an additional column 'Weighted_Sum'.\n",
    "    \"\"\"\n",
    "    # Ensure the weights are in decimal form (e.g., 0.2 for 20%)\n",
    "    weights = {k: v / 100 for k, v in weights.items()}\n",
    "\n",
    "    # Scale each column by its respective weight\n",
    "    df['PTS_Weighted'] = df['PTS'] * weights['PTS']\n",
    "    df['AST_Weighted'] = df['AST'] * weights['AST']\n",
    "    df['TRB_Weighted'] = df['TRB'] * weights['TRB']\n",
    "    \n",
    "    # Create a new column that is the sum of the weighted stats\n",
    "    df['Weighted_Sum'] = df['PTS_Weighted'] + df['AST_Weighted'] + df['TRB_Weighted']\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaled_mvp_stats = scale_and_sum_stats(mvp_stats, feature_importance_percentage)\n",
    "# scaled_mvp_stats.to_csv(\"scaled_mvp_values.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date Divisions:\n",
      "Division 1: 2023-10-24 to 2023-12-20\n",
      "Division 2: 2023-12-21 to 2024-02-16\n",
      "Division 3: 2024-02-17 to 2024-04-14\n",
      "\n",
      "Saving Divisions to CSV Files:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Divisions: 100%|\u001b[92m██████████\u001b[0m| 3/3 [00:00<00:00]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Division 1 saved to first_adjusted.csv\n",
      "Division 2 saved to second_adjusted.csv\n",
      "Division 3 saved to third_adjusted.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# firstSection, secondSection , thirdSection = separate_by_date(scaled_mvp_stats, [\"first_adjusted.csv\", \"second_adjusted.csv\", \"third_adjusted.csv\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def summarize_player_performance(data: pd.DataFrame, sort_by_weighted_sum: bool = False, ascending: bool = False) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Summarizes player performance by calculating the average Weighted_Sum for each player.\n",
    "    Optionally sorts the summarized data by Weighted_Sum.\n",
    "    \n",
    "    Parameters:\n",
    "    - data (pd.DataFrame): Input DataFrame with columns ['player', 'Position', 'Weighted_Sum']\n",
    "    - sort_by_weighted_sum (bool): Whether to sort the summarized data by 'Weighted_Sum'. Default is False.\n",
    "    - ascending (bool): Sort order. True for ascending, False for descending. Default is False.\n",
    "    \n",
    "    Returns:\n",
    "    - pd.DataFrame: Summarized DataFrame with columns ['Player', 'Position', 'Weighted_Sum']\n",
    "      Sorted by 'Weighted_Sum' if sort_by_weighted_sum is True.\n",
    "    \"\"\"\n",
    "    # Validate that necessary columns exist\n",
    "    required_columns = {'Player', 'Position', 'Weighted_Sum'}\n",
    "    if not required_columns.issubset(data.columns):\n",
    "        missing = required_columns - set(data.columns)\n",
    "        raise ValueError(f\"The following required columns are missing from the input data: {missing}\")\n",
    "    \n",
    "    # Handle missing values in key columns\n",
    "    data_clean = data.dropna(subset=['Player', 'Position', 'Weighted_Sum'])\n",
    "    \n",
    "    # Ensure Weighted_Sum is numeric\n",
    "    data_clean['Weighted_Sum'] = pd.to_numeric(data_clean['Weighted_Sum'], errors='coerce')\n",
    "    data_clean = data_clean.dropna(subset=['Weighted_Sum'])\n",
    "    \n",
    "    # Group by player and Position to compute the average Weighted_Sum\n",
    "    summary = data_clean.groupby(['Player', 'Position'], as_index=False)['Weighted_Sum'].mean()\n",
    "    \n",
    "    # Rename 'player' to 'Player' to match desired output\n",
    "    summary.rename(columns={'Player': 'Player'}, inplace=True)\n",
    "    \n",
    "    # Sort the summarized DataFrame if requested\n",
    "    if sort_by_weighted_sum:\n",
    "        summary = summary.sort_values(by='Weighted_Sum', ascending=ascending).reset_index(drop=True)\n",
    "    \n",
    "    return summary\n",
    "\n",
    "\n",
    "def process_three_sections(\n",
    "    section1: pd.DataFrame,\n",
    "    section2: pd.DataFrame,\n",
    "    section3: pd.DataFrame,\n",
    "    ascending: bool = False\n",
    ") -> tuple:\n",
    "    \"\"\"\n",
    "    Processes three player performance DataFrames by summarizing and sorting each.\n",
    "\n",
    "    Parameters:\n",
    "    - section1 (pd.DataFrame): DataFrame for the first season section.\n",
    "    - section2 (pd.DataFrame): DataFrame for the second season section.\n",
    "    - section3 (pd.DataFrame): DataFrame for the third season section.\n",
    "    - ascending (bool): Sort order for Weighted_Sum. True for ascending, False for descending. Default is False.\n",
    "\n",
    "    Returns:\n",
    "    - tuple: A tuple containing three summarized and sorted DataFrames (summary1, summary2, summary3).\n",
    "    \"\"\"\n",
    "    summary1 = summarize_player_performance(\n",
    "        data=section1,\n",
    "        sort_by_weighted_sum=True,\n",
    "        ascending=ascending\n",
    "    )\n",
    "    \n",
    "    summary2 = summarize_player_performance(\n",
    "        data=section2,\n",
    "        sort_by_weighted_sum=True,\n",
    "        ascending=ascending\n",
    "    )\n",
    "    \n",
    "    summary3 = summarize_player_performance(\n",
    "        data=section3,\n",
    "        sort_by_weighted_sum=True,\n",
    "        ascending=ascending\n",
    "    )\n",
    "    \n",
    "    return summary1, summary2, summary3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Player Stats: 100%|\u001b[92m██████████\u001b[0m| 9/9 [00:58<00:00]\n",
      "Normalizing Columns: 100%|\u001b[92m██████████\u001b[0m | 3/3 [00:00<00:00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date Divisions:\n",
      "Division 1: 2023-10-24 to 2023-12-20\n",
      "Division 2: 2023-12-21 to 2024-02-16\n",
      "Division 3: 2024-02-17 to 2024-04-14\n",
      "\n",
      "Saving Divisions to CSV Files:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Divisions: 100%|\u001b[92m██████████\u001b[0m| 3/3 [00:00<00:00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Division 1 saved to first.csv\n",
      "Division 2 saved to second.csv\n",
      "Division 3 saved to third.csv\n",
      "\n",
      "Model Accuracy: 0.41\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 6  1  0  2  1  0  0 14  0]\n",
      " [ 0  1  2  1 11  2  3  0  3]\n",
      " [ 2  0 14  1  2  0  1  1  0]\n",
      " [ 5  0  1  5  1  2  1  7  0]\n",
      " [ 0  0  1  0 19  0  2  1  0]\n",
      " [ 0  0  1  3  3  7  4  0  4]\n",
      " [ 0  0  0  4  5  0 10  0  5]\n",
      " [ 0  0  1  1  0  1  1 21  0]\n",
      " [ 1  2  2  0  5  1  8  1  2]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.43      0.25      0.32        24\n",
      "           2       0.25      0.04      0.07        23\n",
      "           3       0.64      0.67      0.65        21\n",
      "           4       0.29      0.23      0.26        22\n",
      "           5       0.40      0.83      0.54        23\n",
      "           6       0.54      0.32      0.40        22\n",
      "           7       0.33      0.42      0.37        24\n",
      "           8       0.47      0.84      0.60        25\n",
      "           9       0.14      0.09      0.11        22\n",
      "\n",
      "    accuracy                           0.41       206\n",
      "   macro avg       0.39      0.41      0.37       206\n",
      "weighted avg       0.39      0.41      0.37       206\n",
      "\n",
      "Weights (coefficients): [[-0.05057482  2.12475742  3.16140349]\n",
      " [ 0.91066737 -0.24337766 -2.3828792 ]\n",
      " [ 2.27328011  2.95990346  0.37550413]\n",
      " [ 1.26566077 -0.80514859  2.59703798]\n",
      " [ 0.40833222  0.2199028  -4.58580151]\n",
      " [-0.6149669  -2.11328301  0.09396716]\n",
      " [-0.39411159 -1.12377577 -2.72326125]\n",
      " [-3.73065206  1.04004159  4.47757914]\n",
      " [-0.0676351  -2.05902022 -1.01354993]]\n",
      "Intercepts: [-1.82471956  0.57598704 -2.19922444 -0.97901659  1.18644618  1.07884647\n",
      "  1.52360808 -0.56611492  1.20418774]\n",
      "Feature importance:\n",
      "PTS: 9.715880930050394\n",
      "AST: 12.689210524295387\n",
      "TRB: 21.41098380270971\n",
      "Average feature importance:\n",
      "PTS: 1.079542325561155\n",
      "AST: 1.4099122804772652\n",
      "TRB: 2.3789982003010786\n",
      "Feature importance as percentages:\n",
      "PTS: 22.17%\n",
      "AST: 28.96%\n",
      "TRB: 48.87%\n",
      "Date Divisions:\n",
      "Division 1: 2023-10-24 to 2023-12-20\n",
      "Division 2: 2023-12-21 to 2024-02-16\n",
      "Division 3: 2024-02-17 to 2024-04-14\n",
      "\n",
      "Saving Divisions to CSV Files:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Divisions: 100%|\u001b[92m██████████\u001b[0m| 3/3 [00:00<00:00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Division 1 saved to first_adjusted.csv\n",
      "Division 2 saved to second_adjusted.csv\n",
      "Division 3 saved to third_adjusted.csv\n",
      "\n",
      "\n",
      " Averaged per game score per player:\n",
      "                     Player  Position  Weighted_Sum\n",
      "0             Nikola Jokić         1      0.463385\n",
      "1              Luka Dončić         3      0.404270\n",
      "2         Domantas Sabonis         8      0.402250\n",
      "3    Giannis Antetokounmpo         4      0.387814\n",
      "4             Jayson Tatum         6      0.312866\n",
      "5  Shai Gilgeous-Alexander         2      0.302348\n",
      "6             Kevin Durant         9      0.298921\n",
      "7          Anthony Edwards         7      0.259820\n",
      "8            Jalen Brunson         5      0.246055 \n",
      "\n",
      "                     Player  Position  Weighted_Sum\n",
      "0         Domantas Sabonis         8      0.483016\n",
      "1              Luka Dončić         3      0.446423\n",
      "2             Nikola Jokić         1      0.432856\n",
      "3    Giannis Antetokounmpo         4      0.425768\n",
      "4             Jayson Tatum         6      0.327738\n",
      "5  Shai Gilgeous-Alexander         2      0.304351\n",
      "6             Kevin Durant         9      0.303499\n",
      "7            Jalen Brunson         5      0.274335\n",
      "8          Anthony Edwards         7      0.264649 \n",
      "\n",
      "                     Player  Position  Weighted_Sum\n",
      "0             Nikola Jokić         1      0.471983\n",
      "1              Luka Dončić         3      0.456965\n",
      "2         Domantas Sabonis         8      0.454770\n",
      "3    Giannis Antetokounmpo         4      0.432069\n",
      "4             Jayson Tatum         6      0.296900\n",
      "5  Shai Gilgeous-Alexander         2      0.274784\n",
      "6            Jalen Brunson         5      0.271691\n",
      "7          Anthony Edwards         7      0.265591\n",
      "8             Kevin Durant         9      0.262761\n"
     ]
    }
   ],
   "source": [
    "# ### 2024 example\n",
    "\n",
    "# players = [\n",
    "#     \"Nikola Jokić\",\n",
    "#     \"Shai Gilgeous-Alexander\",\n",
    "#     \"Luka Dončić\",\n",
    "#     \"Giannis Antetokounmpo\",\n",
    "#     \"Jalen Brunson\",\n",
    "#     \"Jayson Tatum\",\n",
    "#     \"Anthony Edwards\",\n",
    "#     \"Domantas Sabonis\",\n",
    "#     \"Kevin Durant\"\n",
    "# ]\n",
    "\n",
    "# target_year = 2024\n",
    "# include_playoffs = False\n",
    "\n",
    "# ### everythin above are inputs\n",
    "# ###\n",
    "# ### everything below is the function\n",
    "\n",
    "# mvp_stats = collect_player_stats(players, target_year, include_playoffs, output_file=\"MVP_stats_DIRTY.csv\")\n",
    "# mvp_stats = normalize_player_stats(mvp_stats, 'min-max')\n",
    "# temp1, temp2 , temp3 = separate_by_date(mvp_stats, [\"first.csv\", \"second.csv\", \"third.csv\"])\n",
    "\n",
    "\n",
    "\n",
    "# # Define features and target\n",
    "# X = mvp_stats[['PTS', 'AST', 'TRB']]\n",
    "# y = mvp_stats['Position']\n",
    "\n",
    "# # Split the data\n",
    "# X_train, X_test, y_train, y_test = train_test_split(\n",
    "#     X, y, test_size=0.3, random_state=42, stratify=y\n",
    "# )\n",
    "\n",
    "# # Initialize the Logistic Regression model\n",
    "# model = LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
    "\n",
    "# # Train the model\n",
    "# model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# # Make predictions\n",
    "# y_pred = model.predict(X_test)\n",
    "\n",
    "# # Calculate accuracy\n",
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "# print(f\"\\nModel Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# # Confusion Matrix\n",
    "# conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "# print(\"\\nConfusion Matrix:\")\n",
    "# print(conf_matrix)\n",
    "\n",
    "# # Classification Report\n",
    "# class_report = classification_report(y_test, y_pred)\n",
    "# print(\"\\nClassification Report:\")\n",
    "# print(class_report)\n",
    "\n",
    "# # After training the model, extract the weights (coefficients)\n",
    "# weights = model.coef_\n",
    "# intercepts = model.intercept_\n",
    "\n",
    "# # Print the weights and intercepts\n",
    "# print(\"Weights (coefficients):\", weights)\n",
    "# print(\"Intercepts:\", intercepts)\n",
    "\n",
    "# # Calculate the sum of the absolute values of the coefficients for each feature\n",
    "# feature_importance = abs(weights).sum(axis=0)\n",
    "# importance_dict = dict(zip(['PTS', 'AST', 'TRB'], feature_importance))\n",
    "\n",
    "# # Display the importance of each feature\n",
    "# print(\"Feature importance:\")\n",
    "# for feature, importance in importance_dict.items():\n",
    "#     print(f\"{feature}: {importance}\")\n",
    "\n",
    "\n",
    "# # Calculate the average of the absolute values of the coefficients for each feature\n",
    "# feature_importance_avg = abs(weights).mean(axis=0)\n",
    "# importance_avg_dict = dict(zip(['PTS', 'AST', 'TRB'], feature_importance_avg))\n",
    "\n",
    "# # Display the average importance of each feature\n",
    "# print(\"Average feature importance:\")\n",
    "# for feature, importance in importance_avg_dict.items():\n",
    "#     print(f\"{feature}: {importance}\")\n",
    "\n",
    "# # Normalize the importance to get percentages\n",
    "# total_importance = feature_importance.sum()\n",
    "# feature_importance_percentage = {\n",
    "#     feature: (importance / total_importance) * 100\n",
    "#     for feature, importance in importance_dict.items()\n",
    "# }\n",
    "\n",
    "# # Display the percentage contribution of each feature\n",
    "# print(\"Feature importance as percentages:\")\n",
    "# for feature, percentage in feature_importance_percentage.items():\n",
    "#     print(f\"{feature}: {percentage:.2f}%\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# scaled_mvp_stats = scale_and_sum_stats(mvp_stats, feature_importance_percentage)\n",
    "# scaled_mvp_stats.to_csv(\"scaled_mvp_values.csv\", index=False)\n",
    "\n",
    "# firstSection, secondSection , thirdSection = separate_by_date(scaled_mvp_stats, [\"first_adjusted.csv\", \"second_adjusted.csv\", \"third_adjusted.csv\"])\n",
    "\n",
    "# reducedFirst, reducedSecond, reducedThird = process_three_sections(firstSection, secondSection, thirdSection)\n",
    "\n",
    "# print(\"\\n\\n Averaged per game score per player:\\n\",reducedFirst,\"\\n\\n\", reducedSecond,\"\\n\\n\" ,reducedThird)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    confusion_matrix,\n",
    "    classification_report\n",
    ")\n",
    "\n",
    "# Assuming the following helper functions are defined elsewhere:\n",
    "# collect_player_stats, normalize_player_stats, separate_by_date,\n",
    "# scale_and_sum_stats, process_three_sections\n",
    "\n",
    "def generate_reduced_sections(players, target_year, include_playoffs):\n",
    "    \"\"\"\n",
    "    Processes player statistics to generate reduced datasets for each season section.\n",
    "    \n",
    "    Parameters:\n",
    "    - players (list): List of player names.\n",
    "    - target_year (int): The target season year (e.g., 2024).\n",
    "    - include_playoffs (bool): Whether to include playoff statistics.\n",
    "    \n",
    "    Returns:\n",
    "    - tuple: A tuple containing three DataFrames (reducedFirst, reducedSecond, reducedThird).\n",
    "    \"\"\"\n",
    "    # Step 1: Collect Player Statistics\n",
    "    print(\"Collecting player statistics...\")\n",
    "    mvp_stats = collect_player_stats(\n",
    "        players,\n",
    "        target_year,\n",
    "        include_playoffs,\n",
    "        output_file=\"MVP_stats_DIRTY.csv\"\n",
    "    )\n",
    "    print(\"Player statistics collected.\\n\")\n",
    "    \n",
    "    # Step 2: Normalize Player Statistics\n",
    "    print(\"Normalizing player statistics using Min-Max scaling...\")\n",
    "    mvp_stats = normalize_player_stats(mvp_stats, 'min-max')\n",
    "    print(\"Normalization complete.\\n\")\n",
    "    \n",
    "    # Step 3: Separate Data by Date into Three Sections\n",
    "    print(\"Separating data into three sections based on dates...\")\n",
    "    temp1, temp2, temp3 = separate_by_date(\n",
    "        mvp_stats,\n",
    "        [\"first.csv\", \"second.csv\", \"third.csv\"]\n",
    "    )\n",
    "    print(\"Data separation complete.\\n\")\n",
    "    \n",
    "    # Step 4: Define Features and Target\n",
    "    print(\"Defining features and target variable...\")\n",
    "    X = mvp_stats[['PTS', 'AST', 'TRB']]\n",
    "    y = mvp_stats['Position']\n",
    "    print(\"Features and target defined.\\n\")\n",
    "    \n",
    "    # Step 5: Split the Data into Training and Testing Sets\n",
    "    print(\"Splitting data into training and testing sets...\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X,\n",
    "        y,\n",
    "        test_size=0.3,\n",
    "        random_state=42,\n",
    "        stratify=y\n",
    "    )\n",
    "    print(\"Data splitting complete.\\n\")\n",
    "    \n",
    "    # Step 6: Initialize and Train the Logistic Regression Model\n",
    "    print(\"Initializing and training the Logistic Regression model...\")\n",
    "    model = LogisticRegression(\n",
    "        multi_class='multinomial',\n",
    "        solver='lbfgs',\n",
    "        max_iter=1000  # Increased for convergence\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    print(\"Model training complete.\\n\")\n",
    "    \n",
    "    # Step 7: Make Predictions\n",
    "    print(\"Making predictions on the test set...\")\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(\"Predictions complete.\\n\")\n",
    "    \n",
    "    # Step 8: Evaluate Model Performance\n",
    "    print(\"Evaluating model performance...\")\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"\\nModel Accuracy: {accuracy:.2f}\\n\")\n",
    "    \n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(conf_matrix, \"\\n\")\n",
    "    \n",
    "    class_report = classification_report(y_test, y_pred)\n",
    "    print(\"Classification Report:\")\n",
    "    print(class_report)\n",
    "    \n",
    "    # Step 9: Extract Weights (Coefficients) and Intercepts\n",
    "    print(\"Extracting model coefficients and intercepts...\")\n",
    "    weights = model.coef_\n",
    "    intercepts = model.intercept_\n",
    "    print(\"Weights (coefficients):\", weights)\n",
    "    print(\"Intercepts:\", intercepts, \"\\n\")\n",
    "    \n",
    "    # Step 10: Calculate Feature Importance\n",
    "    print(\"Calculating feature importance based on coefficients...\")\n",
    "    feature_importance = abs(weights).sum(axis=0)\n",
    "    importance_dict = dict(zip(['PTS', 'AST', 'TRB'], feature_importance))\n",
    "    \n",
    "    print(\"Feature importance (sum of absolute coefficients):\")\n",
    "    for feature, importance in importance_dict.items():\n",
    "        print(f\"{feature}: {importance}\")\n",
    "    print()\n",
    "    \n",
    "    feature_importance_avg = abs(weights).mean(axis=0)\n",
    "    importance_avg_dict = dict(zip(['PTS', 'AST', 'TRB'], feature_importance_avg))\n",
    "    \n",
    "    print(\"Average feature importance (mean of absolute coefficients):\")\n",
    "    for feature, importance in importance_avg_dict.items():\n",
    "        print(f\"{feature}: {importance}\")\n",
    "    print()\n",
    "    \n",
    "    # Normalize the importance to get percentages\n",
    "    total_importance = feature_importance.sum()\n",
    "    feature_importance_percentage = {\n",
    "        feature: (importance / total_importance) * 100\n",
    "        for feature, importance in importance_dict.items()\n",
    "    }\n",
    "    \n",
    "    print(\"Feature importance as percentages:\")\n",
    "    for feature, percentage in feature_importance_percentage.items():\n",
    "        print(f\"{feature}: {percentage:.2f}%\")\n",
    "    print()\n",
    "    \n",
    "    # Step 11: Scale and Sum Statistics Based on Feature Importance\n",
    "    print(\"Scaling and summing statistics based on feature importance percentages...\")\n",
    "    scaled_mvp_stats = scale_and_sum_stats(mvp_stats, feature_importance_percentage)\n",
    "    scaled_mvp_stats.to_csv(\"scaled_mvp_values.csv\", index=False)\n",
    "    print(\"Scaling and summing complete. Saved to 'scaled_mvp_values.csv'.\\n\")\n",
    "    \n",
    "    # Step 12: Separate Scaled Data by Date into Three Adjusted Sections\n",
    "    print(\"Separating scaled data into three adjusted sections based on dates...\")\n",
    "    firstSection, secondSection, thirdSection = separate_by_date(\n",
    "        scaled_mvp_stats,\n",
    "        [\"first_adjusted.csv\", \"second_adjusted.csv\", \"third_adjusted.csv\"]\n",
    "    )\n",
    "    print(\"Data separation into adjusted sections complete.\\n\")\n",
    "    \n",
    "    # Step 13: Process the Three Adjusted Sections\n",
    "    print(\"Processing the three adjusted sections to obtain reduced datasets...\")\n",
    "    reducedFirst, reducedSecond, reducedThird = process_three_sections(\n",
    "        firstSection,\n",
    "        secondSection,\n",
    "        thirdSection\n",
    "    )\n",
    "    print(\"Processing complete.\\n\")\n",
    "    \n",
    "    # Optional: Display the reduced datasets\n",
    "    print(\"\\nAveraged per game score per player for Section 1:\")\n",
    "    print(reducedFirst, \"\\n\")\n",
    "    \n",
    "    print(\"Averaged per game score per player for Section 2:\")\n",
    "    print(reducedSecond, \"\\n\")\n",
    "    \n",
    "    print(\"Averaged per game score per player for Section 3:\")\n",
    "    print(reducedThird, \"\\n\")\n",
    "    \n",
    "    # Return the reduced datasets\n",
    "    return reducedFirst, reducedSecond, reducedThird\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching Player Stats: 100%|\u001b[92m██████████\u001b[0m| 9/9 [01:01<00:00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Player statistics collected.\n",
      "\n",
      "Normalizing player statistics using Min-Max scaling...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Normalizing Columns: 100%|\u001b[92m██████████\u001b[0m | 3/3 [00:00<00:00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalization complete.\n",
      "\n",
      "Separating data into three sections based on dates...\n",
      "Date Divisions:\n",
      "Division 1: 2023-10-24 to 2023-12-20\n",
      "Division 2: 2023-12-21 to 2024-02-16\n",
      "Division 3: 2024-02-17 to 2024-04-14\n",
      "\n",
      "Saving Divisions to CSV Files:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Divisions: 100%|\u001b[92m██████████\u001b[0m| 3/3 [00:00<00:00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Division 1 saved to first.csv\n",
      "Division 2 saved to second.csv\n",
      "Division 3 saved to third.csv\n",
      "Data separation complete.\n",
      "\n",
      "Defining features and target variable...\n",
      "Features and target defined.\n",
      "\n",
      "Splitting data into training and testing sets...\n",
      "Data splitting complete.\n",
      "\n",
      "Initializing and training the Logistic Regression model...\n",
      "Model training complete.\n",
      "\n",
      "Making predictions on the test set...\n",
      "Predictions complete.\n",
      "\n",
      "Evaluating model performance...\n",
      "\n",
      "Model Accuracy: 0.41\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 6  1  0  2  1  0  0 14  0]\n",
      " [ 0  1  2  1 11  2  3  0  3]\n",
      " [ 2  0 14  1  2  0  1  1  0]\n",
      " [ 5  0  1  5  1  2  1  7  0]\n",
      " [ 0  0  1  0 19  0  2  1  0]\n",
      " [ 0  0  1  3  3  7  4  0  4]\n",
      " [ 0  0  0  4  5  0 10  0  5]\n",
      " [ 0  0  1  1  0  1  1 21  0]\n",
      " [ 1  2  2  0  5  1  8  1  2]] \n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.43      0.25      0.32        24\n",
      "           2       0.25      0.04      0.07        23\n",
      "           3       0.64      0.67      0.65        21\n",
      "           4       0.29      0.23      0.26        22\n",
      "           5       0.40      0.83      0.54        23\n",
      "           6       0.54      0.32      0.40        22\n",
      "           7       0.33      0.42      0.37        24\n",
      "           8       0.47      0.84      0.60        25\n",
      "           9       0.14      0.09      0.11        22\n",
      "\n",
      "    accuracy                           0.41       206\n",
      "   macro avg       0.39      0.41      0.37       206\n",
      "weighted avg       0.39      0.41      0.37       206\n",
      "\n",
      "Extracting model coefficients and intercepts...\n",
      "Weights (coefficients): [[-0.05057482  2.12475742  3.16140349]\n",
      " [ 0.91066737 -0.24337766 -2.3828792 ]\n",
      " [ 2.27328011  2.95990346  0.37550413]\n",
      " [ 1.26566077 -0.80514859  2.59703798]\n",
      " [ 0.40833222  0.2199028  -4.58580151]\n",
      " [-0.6149669  -2.11328301  0.09396716]\n",
      " [-0.39411159 -1.12377577 -2.72326125]\n",
      " [-3.73065206  1.04004159  4.47757914]\n",
      " [-0.0676351  -2.05902022 -1.01354993]]\n",
      "Intercepts: [-1.82471956  0.57598704 -2.19922444 -0.97901659  1.18644618  1.07884647\n",
      "  1.52360808 -0.56611492  1.20418774] \n",
      "\n",
      "Calculating feature importance based on coefficients...\n",
      "Feature importance (sum of absolute coefficients):\n",
      "PTS: 9.715880930050394\n",
      "AST: 12.689210524295387\n",
      "TRB: 21.41098380270971\n",
      "\n",
      "Average feature importance (mean of absolute coefficients):\n",
      "PTS: 1.079542325561155\n",
      "AST: 1.4099122804772652\n",
      "TRB: 2.3789982003010786\n",
      "\n",
      "Feature importance as percentages:\n",
      "PTS: 22.17%\n",
      "AST: 28.96%\n",
      "TRB: 48.87%\n",
      "\n",
      "Scaling and summing statistics based on feature importance percentages...\n",
      "Scaling and summing complete. Saved to 'scaled_mvp_values.csv'.\n",
      "\n",
      "Separating scaled data into three adjusted sections based on dates...\n",
      "Date Divisions:\n",
      "Division 1: 2023-10-24 to 2023-12-20\n",
      "Division 2: 2023-12-21 to 2024-02-16\n",
      "Division 3: 2024-02-17 to 2024-04-14\n",
      "\n",
      "Saving Divisions to CSV Files:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving Divisions: 100%|\u001b[92m██████████\u001b[0m| 3/3 [00:00<00:00]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Division 1 saved to first_adjusted.csv\n",
      "Division 2 saved to second_adjusted.csv\n",
      "Division 3 saved to third_adjusted.csv\n",
      "Data separation into adjusted sections complete.\n",
      "\n",
      "Processing the three adjusted sections to obtain reduced datasets...\n",
      "Processing complete.\n",
      "\n",
      "\n",
      "Averaged per game score per player for Section 1:\n",
      "                    Player  Position  Weighted_Sum\n",
      "0             Nikola Jokić         1      0.463385\n",
      "1              Luka Dončić         3      0.404270\n",
      "2         Domantas Sabonis         8      0.402250\n",
      "3    Giannis Antetokounmpo         4      0.387814\n",
      "4             Jayson Tatum         6      0.312866\n",
      "5  Shai Gilgeous-Alexander         2      0.302348\n",
      "6             Kevin Durant         9      0.298921\n",
      "7          Anthony Edwards         7      0.259820\n",
      "8            Jalen Brunson         5      0.246055 \n",
      "\n",
      "Averaged per game score per player for Section 2:\n",
      "                    Player  Position  Weighted_Sum\n",
      "0         Domantas Sabonis         8      0.483016\n",
      "1              Luka Dončić         3      0.446423\n",
      "2             Nikola Jokić         1      0.432856\n",
      "3    Giannis Antetokounmpo         4      0.425768\n",
      "4             Jayson Tatum         6      0.327738\n",
      "5  Shai Gilgeous-Alexander         2      0.304351\n",
      "6             Kevin Durant         9      0.303499\n",
      "7            Jalen Brunson         5      0.274335\n",
      "8          Anthony Edwards         7      0.264649 \n",
      "\n",
      "Averaged per game score per player for Section 3:\n",
      "                    Player  Position  Weighted_Sum\n",
      "0             Nikola Jokić         1      0.471983\n",
      "1              Luka Dončić         3      0.456965\n",
      "2         Domantas Sabonis         8      0.454770\n",
      "3    Giannis Antetokounmpo         4      0.432069\n",
      "4             Jayson Tatum         6      0.296900\n",
      "5  Shai Gilgeous-Alexander         2      0.274784\n",
      "6            Jalen Brunson         5      0.271691\n",
      "7          Anthony Edwards         7      0.265591\n",
      "8             Kevin Durant         9      0.262761 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "players = [\n",
    "    \"Nikola Jokić\",\n",
    "    \"Shai Gilgeous-Alexander\",\n",
    "    \"Luka Dončić\",\n",
    "    \"Giannis Antetokounmpo\",\n",
    "    \"Jalen Brunson\",\n",
    "    \"Jayson Tatum\",\n",
    "    \"Anthony Edwards\",\n",
    "    \"Domantas Sabonis\",\n",
    "    \"Kevin Durant\"\n",
    "]\n",
    "\n",
    "target_year = 2024\n",
    "include_playoffs = False\n",
    "\n",
    "\n",
    "TwentyFourfirst,TwentyFoursecond,TwentyFourthird = generate_reduced_sections(players, target_year, include_playoffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_reduced_sections_for_all_years(players_dict, include_playoffs):\n",
    "    \"\"\"\n",
    "    Processes player statistics to generate reduced datasets for each season section for all years.\n",
    "    \n",
    "    Parameters:\n",
    "    - players_dict (dict): Dictionary where keys are years and values are lists of player names.\n",
    "    - include_playoffs (bool): Whether to include playoff statistics.\n",
    "    \n",
    "    Returns:\n",
    "    - dict: A dictionary containing reduced sections for all years.\n",
    "            The keys will be the years, and the values will be tuples (reducedFirst, reducedSecond, reducedThird).\n",
    "    \"\"\"\n",
    "    all_years_data = {}\n",
    "    \n",
    "    # Iterate over each year and its corresponding player list\n",
    "    for year, player_list in players_dict.items():\n",
    "        print(f\"\\nProcessing data for the year {year}...\\n\")\n",
    "        \n",
    "        # Generate the reduced sections for this year\n",
    "        reducedFirst, reducedSecond, reducedThird = generate_reduced_sections(player_list, year, include_playoffs)\n",
    "        \n",
    "        # Store the results in a dictionary\n",
    "        all_years_data[year] = (reducedFirst, reducedSecond, reducedThird)\n",
    "        \n",
    "    return all_years_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sliding_windows(data: dict, window_size: int = 3) -> dict:\n",
    "    \"\"\"\n",
    "    Creates a dictionary of sliding windows from the input data.\n",
    "\n",
    "    Parameters:\n",
    "    - data (dict): Dictionary with years as keys.\n",
    "    - window_size (int): Size of the sliding window (default is 3).\n",
    "\n",
    "    Returns:\n",
    "    - dict: Dictionary of dictionaries representing sliding windows.\n",
    "    \"\"\"\n",
    "    # Sort the years in descending order\n",
    "    sorted_years = sorted(data.keys(), reverse=True)\n",
    "    n = len(sorted_years)\n",
    "    windows = {}\n",
    "\n",
    "    for i in range(n):\n",
    "        window = {}\n",
    "        \n",
    "        # Handle the first window (use pair if not enough left)\n",
    "        if i == 0:\n",
    "            end = i + window_size if (i + window_size) <= n else n\n",
    "            window_years = sorted_years[i:i + window_size]\n",
    "        # Handle the last window (use pair if not enough right)\n",
    "        elif i == n - 1:\n",
    "            start = i - window_size + 1 if (i - window_size + 1) >= 0 else 0\n",
    "            window_years = sorted_years[start:i + 1]\n",
    "        else:\n",
    "            # For middle windows, attempt to take window_size elements\n",
    "            window_years = sorted_years[i:i + window_size]\n",
    "            # If not enough elements to the right, adjust to include more from the left\n",
    "            if len(window_years) < window_size and i >= window_size - 1:\n",
    "                start = i - window_size + 1\n",
    "                window_years = sorted_years[start:i + 1]\n",
    "        \n",
    "        # Ensure window_years has at least two years\n",
    "        if len(window_years) < 2:\n",
    "            # Attempt to include one more year from the left or right\n",
    "            if i > 0:\n",
    "                window_years = sorted_years[i - 1:i + 1]\n",
    "            elif i < n - 1:\n",
    "                window_years = sorted_years[i:i + 2]\n",
    "        \n",
    "        # Create the window dictionary\n",
    "        for year in window_years:\n",
    "            window[year] = data[year]\n",
    "        \n",
    "        # Use the first year in the window as the key\n",
    "        windows[window_years[0]] = window\n",
    "\n",
    "    return windows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024: {2024: ['Nikola Jokić', 'Shai Gilgeous-Alexander', 'Luka Dončić', 'Giannis Antetokounmpo', 'Jalen Brunson', 'Jayson Tatum', 'Anthony Edwards', 'Domantas Sabonis', 'Kevin Durant'], 2023: ['Joel Embiid', 'Nikola Jokić', 'Giannis Antetokounmpo', 'Jayson Tatum', 'Shai Gilgeous-Alexander', 'Donovan Mitchell', 'Domantas Sabonis', 'Luka Dončić', 'Stephen Curry', 'Jimmy Butler', \"De'Aaron Fox\", 'Jalen Brunson', 'Ja Morant'], 2022: ['Nikola Jokić', 'Joel Embiid', 'Giannis Antetokounmpo', 'Devin Booker', 'Luka Dončić', 'Jayson Tatum', 'Ja Morant', 'Stephen Curry', 'Chris Paul', 'DeMar DeRozan', 'Kevin Durant', 'LeBron James']}\n",
      "2023: {2023: ['Joel Embiid', 'Nikola Jokić', 'Giannis Antetokounmpo', 'Jayson Tatum', 'Shai Gilgeous-Alexander', 'Donovan Mitchell', 'Domantas Sabonis', 'Luka Dončić', 'Stephen Curry', 'Jimmy Butler', \"De'Aaron Fox\", 'Jalen Brunson', 'Ja Morant'], 2022: ['Nikola Jokić', 'Joel Embiid', 'Giannis Antetokounmpo', 'Devin Booker', 'Luka Dončić', 'Jayson Tatum', 'Ja Morant', 'Stephen Curry', 'Chris Paul', 'DeMar DeRozan', 'Kevin Durant', 'LeBron James']}\n"
     ]
    }
   ],
   "source": [
    "players = {2024: [\n",
    "    \"Nikola Jokić\",\n",
    "    \"Shai Gilgeous-Alexander\",\n",
    "    \"Luka Dončić\",\n",
    "    \"Giannis Antetokounmpo\",\n",
    "    \"Jalen Brunson\",\n",
    "    \"Jayson Tatum\",\n",
    "    \"Anthony Edwards\",\n",
    "    \"Domantas Sabonis\",\n",
    "    \"Kevin Durant\"\n",
    "]\n",
    ",\n",
    "2023: [\n",
    "    \"Joel Embiid\",\n",
    "    \"Nikola Jokić\",\n",
    "    \"Giannis Antetokounmpo\",\n",
    "    \"Jayson Tatum\",\n",
    "    \"Shai Gilgeous-Alexander\",\n",
    "    \"Donovan Mitchell\",\n",
    "    \"Domantas Sabonis\",\n",
    "    \"Luka Dončić\",\n",
    "    \"Stephen Curry\",\n",
    "    \"Jimmy Butler\",\n",
    "    \"De'Aaron Fox\",\n",
    "    \"Jalen Brunson\",\n",
    "    \"Ja Morant\"\n",
    "],\n",
    "2022: [\n",
    "    \"Nikola Jokić\",\n",
    "    \"Joel Embiid\",\n",
    "    \"Giannis Antetokounmpo\",\n",
    "    \"Devin Booker\",\n",
    "    \"Luka Dončić\",\n",
    "    \"Jayson Tatum\",\n",
    "    \"Ja Morant\",\n",
    "    \"Stephen Curry\",\n",
    "    \"Chris Paul\",\n",
    "    \"DeMar DeRozan\",\n",
    "    \"Kevin Durant\",\n",
    "    \"LeBron James\"\n",
    "], \n",
    "}\n",
    "\n",
    "windows = create_sliding_windows(players, 3, )\n",
    "for key, window in windows.items():\n",
    "    print(f\"{key}: {window}\")\n",
    "\n",
    "# # Example usage:\n",
    "# all_reduced_sections = generate_reduced_sections_for_all_years(players, include_playoffs=False)\n",
    "\n",
    "# # Optional: Display the results\n",
    "# for year, sections in all_reduced_sections.items():\n",
    "#     print(f\"\\nYear: {year}\")\n",
    "#     print(\"Reduced Section 1:\\n\", sections[0])\n",
    "#     print(\"Reduced Section 2:\\n\", sections[1])\n",
    "#     print(\"Reduced Section 3:\\n\", sections[2])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
